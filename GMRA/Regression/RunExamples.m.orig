%
% Script for running several GMRA Regression examples
%

DEG = 2;

COMPAREWITHNOPROJ = false;
COMPARE_NYSTROM   = true;
COMPARE_RSVM      = false;
COMPARE_NN        = true;
<<<<<<< HEAD
COMPARE_RTREES    = false;
=======
COMPARE_RTREES    = true;

DISPLAY = false;
>>>>>>> a1808fa4606f4be9ce4c1e0c442a08fa7dcbd303

%% Setup
SelectDataSetAndOptions                                                                                                         %% Select data set and other options
GenerateTrainAndTestData                                                                                                        %% Generate train and test samples

sigmaY = input('\n Noise level on Y [default=0]:          ');
if isempty(sigmaY), sigmaY = 0; end


%% Set GMRA parameters
SetGMRAParameters
if ~isfield(GMRAopts,'ManifoldDimension') || GMRAopts.ManifoldDimension==0
    GMRAopts.ManifoldDimension = input('\n Intrinsic dimension [default=determined locally by PCA]:          ');
    if isempty(GMRAopts.ManifoldDimension), GMRAopts.ManifoldDimension = 0; end
end

if true
    GMRAopts.CoverTreeBuildOpts.numlevels = int32(100);                                                                         % Go very deep with covertree and GMRA
    GMRAopts.CoverTreeTrimOpts.Size = 5;
end

fprintf('\n The data set X consists of n=%d points in D=%d dimensions, of which %d%% is test.',size(X,2),size(X,1),Params.TestPercent);

% [Q,~]=qr(randn(size(X,1)));
% X_train=Q*X_train;
% X_test=Q*X_test;

%% Construct the Geometric Multi-Resolution Analysis
fprintf('\n Constructing GMRA on train data...');tic
GMRAopts.addTangentialCorrections   = false;
GMRAopts.ComputeWavelets            = false;
%X_train = X_train + 0.01*randn(size(X_train));
%X_test = X_test + 0.01*randn(size(X_test));
gMRA    = GMRA( X_train, GMRAopts );
fprintf('done. (%.4f sec)',toc);

%% Generate the function Y
if ~exist('Labels','var') || isempty(Labels)
    Yfcn    = {@fcn_norm_withspike,@fcn_BallIndicator,@fcn_oscpole,@fcn_norm_withbigspike};
    Y       = [];
    Yo      = [];
    for l = 1:length(Yfcn)
        Ynew    = Yfcn{l}( X );
        Yo      = [Yo,Ynew];
        Y       = [Y,Ynew+sigmaY*randn(size(Ynew))];
    end
    clear Ynew;
else
    sigmaY  = 0;
    Y       = Labels;
    Yo      = Y;
    Yfcn    = {};
end
Y_train = Y(TrainAndTestOpts.TrainIdxs,:);
Y_test  = Yo(TrainAndTestOpts.TestIdxs,:);

%% Perform GMRA regression
fprintf('\n Estimating regression function...');tic
GMRARegressionOpts              = struct('gMRA',gMRA,'GMRAopts',GMRAopts,'degree',[0:DEG],'noProj',false);
if exist('Labels','var') && ~isempty(Labels) && length(unique(Labels))<20                                                       % MM: This is a hack currently for classification...
    GMRARegressionOpts.isClassificationProblem = true;
else
    GMRARegressionOpts.isClassificationProblem = false;
end
GMRARegressionOpts.FcnNames             = cellfun( @(x) func2str(x), Yfcn, 'UniformOutput', false );
gMRAReg                                 = GMRARegression(X_train, Y_train, GMRARegressionOpts );
fprintf('done. (%.4f sec)',toc);

%% Do predictions with regression on uniform partitions
fprintf('\n Evaluating uniform regression estimator at test points...');tic
GMRARegressionTest_opts                 = struct('allscales',true);
Yhat                                    = GMRARegressionTest( gMRAReg, X_test, GMRARegressionTest_opts );
fprintf('done. (%.4f sec)',toc);

%% Do predictions with regression on adaptive partitions
fprintf('\n Evaluating adaptive regression estimator at test points...');tic
GMRARegressionTest_opts                 = struct('allscales',false,'kappa',[],'noProj',GMRARegressionOpts.noProj);
GMRARegressionTest_opts.XGWT            = Yhat.XGWT;
GMRARegressionTest_opts.Scores          = Yhat.Scores;
GMRARegressionTest_opts.ScalCoeffsExt   = Yhat.ScalCoeffsExt;
GMRARegressionTest_opts.Ypreds          = Yhat.Ypreds;
Yhat_adapt                              = GMRARegressionTest( gMRAReg, X_test, GMRARegressionTest_opts );
fprintf('done. (%.4f sec) \n',toc);


if COMPAREWITHNOPROJ
    fprintf('\n Constructing regression without local projections...');
    GMRARegressionOptsNoProj                        = struct('gMRA',gMRA,'GMRAopts',GMRAopts,'degree',[0,1,2],'noProj',false);
    GMRARegressionOptsNoProj.noProj                 = true;
    gMRARegNoProj                                   = GMRARegression(X_train, Y_train, GMRARegressionOptsNoProj );
    GMRARegressionTest_optsNoProj                   = struct('allscales',true);
    YhatNoProj                                      = GMRARegressionTest( gMRARegNoProj, X_test, GMRARegressionTest_optsNoProj );
    GMRARegressionTest_optsNoProj                   = struct('allscales',false,'kappa',[],'noProj',GMRARegressionOptsNoProj.noProj);
    GMRARegressionTest_optsNoProj.XGWT              = YhatNoProj.XGWT;
    GMRARegressionTest_optsNoProj.Scores            = YhatNoProj.Scores;
    GMRARegressionTest_optsNoProj.ScalCoeffsExt     = YhatNoProj.ScalCoeffsExt;
    GMRARegressionTest_optsNoProj.Ypreds            = YhatNoProj.Ypreds;
    Yhat_adaptNoProj                                = GMRARegressionTest( gMRARegNoProj, X_test, GMRARegressionTest_optsNoProj );
    fprintf('\n done. \n');
end


%% Test correctedness of adaptive partitions
if false,
    % Check that there are no overlapping elements in the partition
    for l = 1:length(Yhat_adapt.partition)
        for zk = 1:length(Yhat_adapt.partition{l});
            zpathtoroot=dpath(Y_GMRA.gMRA.cp,Yhat_adapt.partition{l}(zk));
            zidxs=intersect(zpathtoroot(1:end-1),Yhat_adapt.partition{l});
            if ~isempty(zidxs), fprintf('\n\tOverlapping elements in partition %d:',l);zidxs, end
        end
    end
    % Check that the whole space is covered
    for l = 1:length(Yhat_adapt.partition)
        for zk = 1:length(Y_GMRA.gMRA.LeafNodes)
            zpathtoroot=dpath(Y_GMRA.gMRA.cp,Y_GMRA.gMRA.LeafNodes(zk));
            zidxs=intersect(zpathtoroot,Yhat_adapt.partition{l});
            if isempty(zidxs)
                fprintf('\n\tLeaf %d not covered',zk);
            elseif length(zidxs)>1
                fprintf('\n\tLeaf %d covered multiple times',zk);
            end
        end
    end
end

%% Compute prediction error
if ~gMRAReg.RegressionOpts.isClassificationProblem
    unifError   = ComputeRegressionError(Y_test,Yhat);
    adaptError  = ComputeRegressionError(Y_test,Yhat_adapt);
else                                                                                                                            % It's probably a classification problem
    unifError   = ComputeRegressionError(Y_test,Yhat,unique(Y));
    adaptError  = ComputeRegressionError(Y_test,Yhat_adapt,unique(Y));
end
if COMPAREWITHNOPROJ
    if ~gMRAReg.RegressionOpts.isClassificationProblem
        unifErrorNoProj   = ComputeRegressionError(Y_test,YhatNoProj);
        adaptErrorNoProj  = ComputeRegressionError(Y_test,Yhat_adaptNoProj);
    else                                                                                                                            % It's probably a classification problem
        unifErrorNoProj   = ComputeRegressionError(Y_test,YhatNoProj,unique(Y));
        adaptErrorNoProj  = ComputeRegressionError(Y_test,Yhat_adaptNoProj,unique(Y));
    end
end


%% Visualizes the best partitions
if true
    [~,minidx] = min(adaptError.rel(1,:));
    GMRA_VisualizePartition(X_train,gMRA,Yhat_adapt.partition{2}(1:10));
end

drawnow;
%[zzX,zzY,zzT,zzAUC] = perfcurve(Y_test(:,1),Yhat.Yhat{1}{end}(:,1)/max(Yhat.Yhat{1}{end}(:,1)),1);figure;plot(zzX,zzY);

%% Compare with regression trees
fprintf('\n Constructing regression trees...');
clear Y_test_regtree Error_regTree RegTree
if COMPARE_RTREES
    fprintf('\n Constructing regression trees...');
    for k = size(Y,2):-1:1
        if ~gMRAReg.RegressionOpts.isClassificationProblem
            RegTree{k} = fitrtree(X_train',Y_train(:,k),'CrossVal','off');
            Y_test_regtree(:,k) = predict(RegTree{k},X_test');
            Error_regTree(k) = norm(Y_test(:,k)-Y_test_regtree(:,k))/norm(Y_test(:,k));
        else
            RegTree{k} = fitctree(X_train',Y_train(:,k),'CrossVal','off');
            Y_test_regtree(:,k) = predict(RegTree{k},X_test');
            Error_regTree(k) = sum(Y_test(:,k)~=Y_test_regtree(:,k))/size(Y_test,1);
        end
    end
    fprintf('done.\n');
end

%% Compare with NN
clear Y_testNN Error_NN
if COMPARE_NN
    fprintf('\n Performing NN regression...');
    for k = size(Y,2):-1:1,
        if isfield(gMRA,'CoverTree'),
            Y_testNN{k} = NNclassify( X_train, Y_train(:,k), X_test, 1:20, gMRA.CoverTree, gMRAReg.RegressionOpts.isClassificationProblem );
        else
            Y_testNN{k} = NNclassify( X_train, Y_train(:,k), X_test, 1:20, [], gMRAReg.RegressionOpts.isClassificationProblem );
        end
        if gMRAReg.RegressionOpts.isClassificationProblem
            for l = size(Y_testNN{k},2):-1:1
                Error_NN(l,k) = sum(Y_testNN{k}(:,l)~=Y_test(:,k))/length(Y_test(:,k));
            end
        else
            for l = size(Y_testNN{k},2):-1:1
                Error_NN(l,k) = norm(Y_testNN{k}(:,l)-Y_test(:,k))/norm(Y_test(:,k));
            end
        end
    end
    fprintf('done.\n');
end


%% Compare with Ridge regression a la Nystrom
if COMPARE_NYSTROM
    fprintf('\n Performing ridge regression...');
    clear nystromCoRe_config nystromCoRe_training_output nystromCoRe_prediction_output Error_nystromCoRe
    [medianmindist,medianmediandist,minmindist] = EstimateMinPairwiseDistance( X );
    nystrcomCoRe_kernelParameter                = linspace(minmindist,medianmediandist/2,5);
    for l = 1:length(nystrcomCoRe_kernelParameter)
        nystromCoRe_config(l)                   = config_set( 'recompute', 1, 'kernel.kernelParameter',nystrcomCoRe_kernelParameter(l) );
        for k = size(Y,2):-1:1
            nystromCoRe_training_output(k,l)    = nystromCoRe_train ( X_train' , Y_train(:,k), nystromCoRe_config(l) );
            nystromCoRe_prediction_output(k,l)  = nystromCoRe_test  ( X_test' , Y_test(:,k) , nystromCoRe_training_output(k,l) );
            Error_nystromCoRe(l,k)              = norm(nystromCoRe_prediction_output(k,l).YtePred-Y_test(:,k))/norm(Y_test(:,k));
        end
    end
    fprintf('done.');
    %figure;plot(nystrcomCoRe_kernelParameter,log10(Error_nystromCoRe'));axis tight;xlabel('\sigma');
end

%% Now compare with regression SVM
if COMPARE_RSVM
    fprintf('\n Constructing regression SVM''s...');
    clear ParamsRSVM ParamRSVMList RSVM Y_test_RSVM Error_RSVM
    ParamsRSVM{1}.Name     = 'KernelFunction';
    ParamsRSVM{1}.Values   = {'linear','gaussian','rbf','polynomial'};
    ParamsRSVM{2}.Name     = 'PolynomialOrder';
    ParamsRSVM{2}.Values   = {1};
    ParamRSVMList          = GenerateStructuresWithVariedParameters( ParamsRSVM );
    L = length(ParamRSVMList);
    for l = 1:L
        if strcmp(ParamRSVMList{l}.KernelFunction,'polynomial')
            for p = 2:3
                ParamRSVMList{end+1} = ParamRSVMList{l};
                ParamRSVMList{end}.PolynomialOrder = p;
            end
        end
    end
    
    for l = 1:length(ParamRSVMList)
        for k = size(Y,2):-1:1
            if strcmp(ParamRSVMList{l}.KernelFunction,'polynomial')
                RSVM{l,k} = fitrsvm(X_train',Y_train(:,k),'KernelFunction',ParamRSVMList{l}.KernelFunction,'BoxConstraint',max(abs(Y_train(:,k))),'PolynomialOrder',ParamRSVMList{l}.PolynomialOrder,'CrossVal','off');
            else
                RSVM{l,k} = fitrsvm(X_train',Y_train(:,k),'KernelFunction',ParamRSVMList{l}.KernelFunction,'BoxConstraint',max(abs(Y_train(:,k))),'CrossVal','off');
            end
            Y_test_RSVM(:,l,k) = predict(RSVM{l,k},X_test');
            Error_RSVM(l,k) = norm(Y_test(:,k)-squeeze(Y_test_RSVM(:,l,k)))/norm(Y_test(:,k));
        end
    end
    fprintf('done.');
    Error_RSVM
end

%% Numerical results
for k = 1:size(Y_test,2)
<<<<<<< HEAD
    fprintf('\n SUMMARY of performance for function %d --------------------------------------------------------',k)
    fprintf('\n Best result from multiscale uniform estimators, of varying degrees: %f',min(squeeze(unifError.rel(:,:,k)),[],2)')
    fprintf('\n Best result from multiscale adaptive estimators, of varying degrees: %f',min(squeeze(adaptError.rel(:,:,k)),[],2)')
    if COMPARE_RTREES, fprintf('\n Best result from regression and classification trees: %f',Error_regTree(k)); end
    if COMPARE_NN, fprintf('\n Best result from NN estimators: %f',min(squeeze(Error_NN(:,k)))); end
    if COMPARE_NYSTROM, fprintf('\n Best result from NystromCoRe: %f',min(squeeze(Error_nystromCoRe(k,:)))); end
=======
    fprintf('\n\n SUMMARY of performance for function %d --------------------------------------------------------',k)
    fprintf('\n Best result from multiscale uniform estimators of degree %d: %f',[0:DEG;min(squeeze(unifError.rel(:,:,k)),[],2)'])
    fprintf('\n Best result from multiscale adaptive estimators of degree %d: %f',[0:DEG;min(squeeze(adaptError.rel(:,:,k)),[],2)'])
    if COMPAREWITHNOPROJ
        fprintf('\n Best result from multiscale uniform non-projected estimators of degree %d: %f',[0:DEG;min(squeeze(unifErrorNoProj.rel(:,:,k)),[],2)'])
        fprintf('\n Best result from multiscale adaptive non-projected estimators of degree %d: %f',[0:DEG;min(squeeze(adaptErrorNoProj.rel(:,:,k)),[],2)'])
    end
    if COMPARE_RTREES
        fprintf('\n Best result from regression and classification trees: %f',Error_regTree(k));
    end
    if COMPARE_NN
        fprintf('\n Best result from NN estimators: %f',min(squeeze(Error_NN(:,k))));
    end
    if COMPARE_NYSTROM
        fprintf('\n Best result from NystromCoRe: %f',min(squeeze(Error_nystromCoRe(k,:))));
    end
>>>>>>> a1808fa4606f4be9ce4c1e0c442a08fa7dcbd303
end

fprintf('\n');

clear competitors

competitors     = struct('alg',[],'err',[],'col',[]);
competitors.alg = {'CART','Nearest Neighbors','Nystrom'};
competitors.err = {Error_regTree,min(Error_NN,[],1),min(Error_nystromCoRe,[],1)};
competitors.col = {'m', 'c' , 'g',[0.7,0.4,0],[0.8,0.5,0],[0.9,0.6,0],[1,0.7,0]};

if COMPAREWITHNOPROJ
    competitors.alg = [competitors.alg,{'NoProj linear unif.','NoProj linear adapt.','NoProj quad. unif.','NoProj quad. adapt.'}];
    competitors.err = [competitors.err,{min(squeeze(unifErrorNoProj.abs(2,:,:)),[],1),min(squeeze(adaptErrorNoProj.abs(2,:,:)),[],1),min(squeeze(unifErrorNoProj.abs(3,:,:)),[],1),min(squeeze(adaptErrorNoProj.abs(3,:,:)),[],1)}];
end


%% Display accuracy
if DISPLAY
    DisplayRegressionErrorVsPartitionSize( X_test,gMRAReg, Y_test, Yhat, Yhat_adapt, unifError, adaptError, sigmaY , competitors);
end

% [zzX,zzY,zzT,zzAUC] = perfcurve(Y_test(:,1),Yhat.Yhat{1}{end}(:,1)/max(Yhat.Yhat{1}{end}(:,1)),1);figure;plot(zzX,zzY);

save('results')
