{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy.io as scio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_data = scio.loadmat('cifar10_low_dim.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_data['low_dim'][0,0][0,6].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## displaying the number of dimension for each scale for scale 0 to 6 (rough to fine scale)\n",
    "for example: at scale 2, the images' wavelet coefficient on the gMRA ranging from 4 to 50, most of which has 50 coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 0 num_sizes: 1, distribution: {50: 50000}\n",
      "scale 1 num_sizes: 1, distribution: {50: 50000}\n",
      "scale 2 num_sizes: 8, distribution: {50: 46066, 12: 359, 46: 2123, 11: 628, 16: 525, 7: 214, 5: 69, 4: 16}\n",
      "scale 3 num_sizes: 22, distribution: {50: 35784, 32: 1416, 12: 1074, 28: 933, 8: 381, 16: 308, 10: 1236, 29: 606, 9: 488, 23: 601, 45: 1079, 6: 557, 13: 358, 17: 579, 46: 1600, 11: 214, 4: 187, 5: 371, 7: 502, 31: 433, 40: 888, 21: 405}\n",
      "scale 4 num_sizes: 39, distribution: {9: 2534, 10: 1992, 11: 1808, 6: 1429, 7: 1704, 8: 2259, 20: 928, 18: 947, 46: 1576, 2: 422, 4: 927, 12: 1480, 16: 592, 5: 1225, 15: 511, 38: 632, 47: 792, 3: 168, 1: 116, 17: 742, 13: 908, 40: 843, 19: 517, 50: 14228, 21: 606, 34: 986, 43: 606, 30: 1227, 24: 832, 32: 1562, 23: 655, 37: 1048, 14: 584, 22: 267, 36: 355, 27: 225, 26: 631, 25: 545, 44: 591}\n",
      "scale 5 num_sizes: 38, distribution: {6: 5283, 4: 1441, 12: 1906, 21: 1026, 5: 3800, 9: 3354, 3: 169, 7: 5442, 22: 636, 11: 2722, 2: 514, 1: 835, 20: 934, 8: 3698, 13: 1552, 14: 1500, 17: 1851, 10: 2592, 31: 161, 18: 1617, 19: 1380, 34: 200, 16: 1345, 26: 264, 15: 1612, 27: 302, 24: 571, 25: 691, 23: 674, 36: 79, 43: 204, 30: 269, 28: 342, 33: 239, 32: 224, 29: 296, 35: 204, 37: 71}\n",
      "scale 6 num_sizes: 43, distribution: {23: 1893, 13: 3312, 20: 2220, 12: 2493, 21: 2440, 10: 1649, 14: 3375, 9: 1211, 15: 3722, 6: 235, 22: 1509, 18: 3079, 8: 931, 11: 2599, 16: 3604, 17: 3246, 7: 488, 19: 2454, 27: 684, 24: 1598, 25: 974, 31: 448, 32: 548, 34: 433, 26: 622, 5: 156, 35: 456, 28: 682, 29: 338, 38: 99, 36: 203, 33: 391, 30: 570, 3: 18, 50: 728, 48: 108, 44: 54, 43: 38, 46: 46, 40: 126, 41: 54, 4: 36, 37: 130}\n"
     ]
    }
   ],
   "source": [
    "low_dim = cifar_data['low_dim']\n",
    "all_scales = {}\n",
    "num_scale = low_dim[0][0].shape[1]\n",
    "for i in range(num_scale):\n",
    "    scale_i = []\n",
    "    for j in range(low_dim.shape[1]):\n",
    "        p = low_dim[0][j][0][i]\n",
    "        if p.size != 0:\n",
    "            p = p.reshape(p.shape[0])\n",
    "        else:\n",
    "            p = all_scales[i-1][j]\n",
    "        scale_i.append(p)\n",
    "\n",
    "    all_scales[i] = np.array(scale_i)\n",
    "for i in range(7):\n",
    "    scale_i_sizes = {}\n",
    "#     size_scale_i = all_scales[i][0].shape[0]\n",
    "    for j in range(low_dim.shape[1]):\n",
    "        size_j = all_scales[i][j].shape[0]\n",
    "        if not scale_i_sizes.get(size_j):\n",
    "            scale_i_sizes[size_j] = 1\n",
    "        else:\n",
    "            scale_i_sizes[size_j] += 1\n",
    "    print(f\"scale {i} num_sizes: {len(scale_i_sizes)}, distribution: {scale_i_sizes}\")\n",
    "#         if all_scales[i][j].shape[0] != size_scale_i:\n",
    "#             print(\"point not equal size: \",i,j, all_scales[i][j].shape[0], size_scale_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data\n",
    "for each scale, backfilling all datapoints so that within each scale, all datapoints are in the same # of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_for_scale = {}\n",
    "for i in range(7):\n",
    "    size_scale_i = all_scales[i][0].shape[0]\n",
    "    for j in range(low_dim.shape[1]):\n",
    "        size_j = all_scales[i][j].shape[0]\n",
    "        size_scale_i = max(size_j, size_scale_i)\n",
    "    max_size_for_scale[i] = size_scale_i\n",
    "print(max_size_for_scale)\n",
    "for i in range(len(all_scales)):\n",
    "    embedded_scale_i = []\n",
    "    for j in range(low_dim.shape[1]):\n",
    "        embedded_vect = all_scales[i][j]\n",
    "        if all_scales[i][j].shape[0] < max_size_for_scale[i]:\n",
    "            embedded_vect = np.zeros((max_size_for_scale[i],))\n",
    "            embedded_vect[:all_scales[i][j].shape[0]] = all_scales[i][j]\n",
    "        embedded_scale_i.append(embedded_vect)\n",
    "    all_scales[i] = np.asarray(embedded_scale_i)\n",
    "np.save(\"cifar10_normalized_by_scale\", all_scales, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = np.load('cifar10_normalized_by_scale.npy',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "cifar_labels = scio.loadmat('cifar_10_labels.mat')\n",
    "labels = cifar_labels['Labels']\n",
    "Labels = labels.reshape(labels.shape[0])\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying with LeNet\n",
    "images are represented by hyperplane coefficients and classified with LENET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, (2,2), padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (2,2))\n",
    "        self.fc1   = nn.Linear(16*2, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((8,5,10)).unsqueeze(1)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, (2,2))\n",
    "        x = nn.functional.max_pool2d(nn.functional.relu(self.conv2(x)), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "class ShallowNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ShallowNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out =self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 50 # low dimension resolution\n",
    "hidden_size = 20 #\n",
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 8\n",
    "learning_rate = 10e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NN\n",
    "training the neural network with low dimension image representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-d29eb511f23b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m                           \u001b[0;31m# Forward pass: compute the output class given a image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# Compute the loss: difference between the output class and the pre-given label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                   \u001b[0;31m# Backward pass: compute the weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                  \u001b[0;31m# Optimizer: update the weights of hidden nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#                 if (i+1) % 1000 == 0:                              # Logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training network with rough scale representation (scale 0)\n",
    "scale_i_data = np.matrix(load_data.item()[0])\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(scale_i_data, Labels)\n",
    "(train_acc, test_acc) = (0,0)\n",
    "for train_index, test_index in kf.split(scale_i_data,Labels):\n",
    "#         getting train & test data of k fold & normalize input\n",
    "    X_train, X_test = scale_i_data[train_index], scale_i_data[test_index]\n",
    "    X_train -= X_train.min()\n",
    "    X_train /= X_train.max()\n",
    "    X_test -= X_test.min()\n",
    "    X_test /= X_test.max()\n",
    "    y_train, y_test = Labels[train_index], Labels[test_index]\n",
    "#         initialize network\n",
    "    net = ShallowNetwork(input_size, hidden_size, num_classes).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#         convert data to tensors\n",
    "    X_train_tensor = torch.Tensor(X_train).cuda()\n",
    "    y_train_tensor = torch.Tensor(y_train).cuda()\n",
    "    train_dataset = data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,)\n",
    "    X_test_tensor = torch.Tensor(X_test).cuda()\n",
    "    y_test_tensor = torch.Tensor(y_test).cuda()\n",
    "    test_dataset = data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,)\n",
    "#         Train network\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            images = Variable(images).cuda()         \n",
    "            labels = Variable(labels.long()).cuda()\n",
    "            optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "            outputs = net(images)                           # Forward pass: compute the output class given a image\n",
    "            loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "            loss.backward()                                   # Backward pass: compute the weight\n",
    "            optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "#                 if (i+1) % 1000 == 0:                              # Logging\n",
    "#                     print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "#                  %(epoch+1, num_epochs, i+1, X_train.shape[0]//batch_size, loss.data))\n",
    "#         Test on test images\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.long()\n",
    "        images = Variable(images)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "        total += labels.size(0)                    # Increment the total count\n",
    "        correct += (predicted == labels).sum()     # Increment the correct count\n",
    "    test_accuracy = 100 * correct / total\n",
    "#         Test on train images\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels = labels.long()\n",
    "        images = Variable(images)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "        total += labels.size(0)                    # Increment the total count\n",
    "        correct += (predicted == labels).sum()     # Increment the correct count\n",
    "    train_accuracy = 100* correct / total\n",
    "    print(f'Training Accuracy {train_accuracy:.2f}, Testing Accuracy images: {test_accuracy:.2f}')\n",
    "    train_acc += train_accuracy\n",
    "    test_acc += test_accuracy\n",
    "print(f\"##############Average k-fold accuracy##############\")\n",
    "print(f\"Training Accuracy {(train_acc/10):.2f}, Testing Accuracy images: {(test_acc/10):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
