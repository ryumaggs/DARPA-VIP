{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "\n",
    "import scipy.io as scio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 11 # low dimension resolution\n",
    "hidden_size = 50 #\n",
    "num_classes = 10\n",
    "num_epochs = 30\n",
    "batch_size = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ShallowNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000, 11])\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Creating TrainLoader\n",
    "# load low_dim training data\n",
    "low_dim_data = scio.loadmat('low_dim_6000.mat')\n",
    "low_dim = low_dim_data['low_dim'].reshape(6000)\n",
    "low_dim = np.array([i.reshape(11) for i in low_dim])\n",
    "low_dim -= low_dim.min()\n",
    "low_dim /= low_dim.max()\n",
    "# load labels\n",
    "orig_data = scio.loadmat('subset_6000.mat')\n",
    "labels = orig_data['subset_label_X']\n",
    "labels = labels.reshape(6000)\n",
    "# convert to dataLoader\n",
    "\n",
    "tensor_low_dim = torch.Tensor(low_dim)#[:5000,:])\n",
    "tensor_labels = torch.Tensor(labels)#[:5000])\n",
    "low_dim_dataset = data.TensorDataset(tensor_low_dim, tensor_labels)\n",
    "train_loader = data.DataLoader(dataset=low_dim_dataset,\n",
    "                              batch_size=batch_size,)\n",
    "print(tensor_low_dim.shape)\n",
    "\n",
    "\n",
    "\n",
    "###### load test dataset #####\n",
    "\n",
    "test_data = scio.loadmat('low_dim_orig_test.mat')\n",
    "test_dat = test_data['low_dim'].reshape(10000)\n",
    "test_dat = np.array([i.reshape(18) for i in test_dat])\n",
    "test_dat = np.array([i[0].reshape(11) for i in test_dat])\n",
    "test_dat -= test_dat.min()\n",
    "test_dat /= test_dat.max()\n",
    "test_labels = np.load('test_labels.npy')\n",
    "print(len(test_dat))\n",
    "# custom_train\n",
    "train_dat = torch.Tensor(test_dat[:9000,:])\n",
    "train_labels = torch.Tensor(test_labels[:9000])\n",
    "train_custom_dataset = data.TensorDataset(train_dat, train_labels)\n",
    "custom_train_loader = data.DataLoader(dataset=train_custom_dataset,\n",
    "                              batch_size=batch_size,)\n",
    "\n",
    "# custom_test\n",
    "\n",
    "custom_test_dat = torch.Tensor(test_dat[9000:,:])\n",
    "custom_test_labels = torch.Tensor(test_labels[9000:])\n",
    "test_custom_dataset = data.TensorDataset(custom_test_dat, custom_test_labels)\n",
    "custom_test_loader = data.DataLoader(dataset=test_custom_dataset,\n",
    "                              batch_size=batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 11])\n"
     ]
    }
   ],
   "source": [
    "# Creating TestLoader\n",
    "# load_low_dim test data\n",
    "low_dim_data = scio.loadmat('low_dim_test_1000.mat')\n",
    "low_dim = low_dim_data['low_dim_test'].reshape(1000)\n",
    "low_dim = np.array([i.reshape(11) for i in low_dim])\n",
    "low_dim -= low_dim.min()\n",
    "low_dim /= low_dim.max()\n",
    "# load labels\n",
    "labels = low_dim_data['subset_label_X_test']\n",
    "labels = labels.reshape(1000)\n",
    "# # convert to dataLoader\n",
    "tensor_low_dim = torch.Tensor(low_dim[:,:])\n",
    "tensor_labels = torch.Tensor(labels[:])\n",
    "low_dim_dataset = data.TensorDataset(tensor_low_dim, tensor_labels)\n",
    "test_loader = data.DataLoader(dataset=low_dim_dataset,\n",
    "                              batch_size=batch_size,)\n",
    "print(tensor_low_dim.shape)\n",
    "\n",
    "\n",
    "###### load adv test dataset #####\n",
    "\n",
    "adv_data = scio.loadmat('low_dim_adv_test.mat')\n",
    "adv_dat = adv_data['low_dim'].reshape(10000)\n",
    "adv_dat = np.array([i.reshape(18) for i in adv_dat])\n",
    "adv_dat = np.array([i[0].reshape(11) for i in adv_dat])\n",
    "adv_dat -= adv_dat.min()\n",
    "adv_dat /= adv_dat.max()\n",
    "adv_labels = np.load('test_labels.npy')\n",
    "# custom_test\n",
    "adv_dat = torch.Tensor(adv_dat[9000:,:])\n",
    "adv_labels = torch.Tensor(adv_labels[9000:])\n",
    "test_adv_dataset = data.TensorDataset(adv_dat, adv_labels)\n",
    "custom_adv_loader = data.DataLoader(dataset=test_adv_dataset,\n",
    "                              batch_size=batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15.288879],\n",
       "       [  24.870827],\n",
       "       [-295.22333 ],\n",
       "       [ 290.0678  ],\n",
       "       [-181.30225 ],\n",
       "       [  66.25981 ],\n",
       "       [ 617.94574 ],\n",
       "       [-135.3153  ],\n",
       "       [-323.53192 ],\n",
       "       [ 475.9786  ],\n",
       "       [  58.22062 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_dim_data['low_dim_test'][0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ShallowNetwork(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.cuda()    # You can comment out this line to disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [100/600], Loss: 2.2663\n",
      "Epoch [1/30], Step [200/600], Loss: 2.2413\n",
      "Epoch [1/30], Step [300/600], Loss: 1.6579\n",
      "Epoch [1/30], Step [400/600], Loss: 1.6944\n",
      "Epoch [1/30], Step [500/600], Loss: 1.1110\n",
      "Epoch [1/30], Step [600/600], Loss: 1.3602\n",
      "Epoch [1/30], Step [700/600], Loss: 1.5869\n",
      "Epoch [1/30], Step [800/600], Loss: 1.4300\n",
      "Epoch [1/30], Step [900/600], Loss: 1.1025\n",
      "Epoch [2/30], Step [100/600], Loss: 1.1228\n",
      "Epoch [2/30], Step [200/600], Loss: 1.7108\n",
      "Epoch [2/30], Step [300/600], Loss: 0.8638\n",
      "Epoch [2/30], Step [400/600], Loss: 1.0836\n",
      "Epoch [2/30], Step [500/600], Loss: 0.6665\n",
      "Epoch [2/30], Step [600/600], Loss: 0.8907\n",
      "Epoch [2/30], Step [700/600], Loss: 1.4223\n",
      "Epoch [2/30], Step [800/600], Loss: 1.2303\n",
      "Epoch [2/30], Step [900/600], Loss: 0.7937\n",
      "Epoch [3/30], Step [100/600], Loss: 0.9497\n",
      "Epoch [3/30], Step [200/600], Loss: 1.5360\n",
      "Epoch [3/30], Step [300/600], Loss: 0.7206\n",
      "Epoch [3/30], Step [400/600], Loss: 0.9724\n",
      "Epoch [3/30], Step [500/600], Loss: 0.5413\n",
      "Epoch [3/30], Step [600/600], Loss: 0.6769\n",
      "Epoch [3/30], Step [700/600], Loss: 1.2638\n",
      "Epoch [3/30], Step [800/600], Loss: 1.2307\n",
      "Epoch [3/30], Step [900/600], Loss: 0.7077\n",
      "Epoch [4/30], Step [100/600], Loss: 0.9522\n",
      "Epoch [4/30], Step [200/600], Loss: 1.4542\n",
      "Epoch [4/30], Step [300/600], Loss: 0.5958\n",
      "Epoch [4/30], Step [400/600], Loss: 0.8963\n",
      "Epoch [4/30], Step [500/600], Loss: 0.4268\n",
      "Epoch [4/30], Step [600/600], Loss: 0.5391\n",
      "Epoch [4/30], Step [700/600], Loss: 1.0983\n",
      "Epoch [4/30], Step [800/600], Loss: 1.1405\n",
      "Epoch [4/30], Step [900/600], Loss: 0.6726\n",
      "Epoch [5/30], Step [100/600], Loss: 0.9771\n",
      "Epoch [5/30], Step [200/600], Loss: 1.3934\n",
      "Epoch [5/30], Step [300/600], Loss: 0.5438\n",
      "Epoch [5/30], Step [400/600], Loss: 0.7959\n",
      "Epoch [5/30], Step [500/600], Loss: 0.3439\n",
      "Epoch [5/30], Step [600/600], Loss: 0.4696\n",
      "Epoch [5/30], Step [700/600], Loss: 0.9889\n",
      "Epoch [5/30], Step [800/600], Loss: 1.0988\n",
      "Epoch [5/30], Step [900/600], Loss: 0.6422\n",
      "Epoch [6/30], Step [100/600], Loss: 1.0126\n",
      "Epoch [6/30], Step [200/600], Loss: 1.3181\n",
      "Epoch [6/30], Step [300/600], Loss: 0.5167\n",
      "Epoch [6/30], Step [400/600], Loss: 0.7421\n",
      "Epoch [6/30], Step [500/600], Loss: 0.2818\n",
      "Epoch [6/30], Step [600/600], Loss: 0.4147\n",
      "Epoch [6/30], Step [700/600], Loss: 0.9335\n",
      "Epoch [6/30], Step [800/600], Loss: 1.0687\n",
      "Epoch [6/30], Step [900/600], Loss: 0.5880\n",
      "Epoch [7/30], Step [100/600], Loss: 1.0036\n",
      "Epoch [7/30], Step [200/600], Loss: 1.2311\n",
      "Epoch [7/30], Step [300/600], Loss: 0.4890\n",
      "Epoch [7/30], Step [400/600], Loss: 0.6524\n",
      "Epoch [7/30], Step [500/600], Loss: 0.2454\n",
      "Epoch [7/30], Step [600/600], Loss: 0.3947\n",
      "Epoch [7/30], Step [700/600], Loss: 0.8337\n",
      "Epoch [7/30], Step [800/600], Loss: 1.0568\n",
      "Epoch [7/30], Step [900/600], Loss: 0.4778\n",
      "Epoch [8/30], Step [100/600], Loss: 1.0144\n",
      "Epoch [8/30], Step [200/600], Loss: 1.1567\n",
      "Epoch [8/30], Step [300/600], Loss: 0.4453\n",
      "Epoch [8/30], Step [400/600], Loss: 0.5948\n",
      "Epoch [8/30], Step [500/600], Loss: 0.2041\n",
      "Epoch [8/30], Step [600/600], Loss: 0.3913\n",
      "Epoch [8/30], Step [700/600], Loss: 0.7169\n",
      "Epoch [8/30], Step [800/600], Loss: 1.0895\n",
      "Epoch [8/30], Step [900/600], Loss: 0.3904\n",
      "Epoch [9/30], Step [100/600], Loss: 0.9877\n",
      "Epoch [9/30], Step [200/600], Loss: 1.1249\n",
      "Epoch [9/30], Step [300/600], Loss: 0.4053\n",
      "Epoch [9/30], Step [400/600], Loss: 0.5361\n",
      "Epoch [9/30], Step [500/600], Loss: 0.1628\n",
      "Epoch [9/30], Step [600/600], Loss: 0.3496\n",
      "Epoch [9/30], Step [700/600], Loss: 0.6249\n",
      "Epoch [9/30], Step [800/600], Loss: 1.1243\n",
      "Epoch [9/30], Step [900/600], Loss: 0.3393\n",
      "Epoch [10/30], Step [100/600], Loss: 0.9358\n",
      "Epoch [10/30], Step [200/600], Loss: 1.1068\n",
      "Epoch [10/30], Step [300/600], Loss: 0.3932\n",
      "Epoch [10/30], Step [400/600], Loss: 0.4910\n",
      "Epoch [10/30], Step [500/600], Loss: 0.1432\n",
      "Epoch [10/30], Step [600/600], Loss: 0.3216\n",
      "Epoch [10/30], Step [700/600], Loss: 0.5457\n",
      "Epoch [10/30], Step [800/600], Loss: 1.1805\n",
      "Epoch [10/30], Step [900/600], Loss: 0.3100\n",
      "Epoch [11/30], Step [100/600], Loss: 0.9468\n",
      "Epoch [11/30], Step [200/600], Loss: 1.1031\n",
      "Epoch [11/30], Step [300/600], Loss: 0.3692\n",
      "Epoch [11/30], Step [400/600], Loss: 0.4478\n",
      "Epoch [11/30], Step [500/600], Loss: 0.1426\n",
      "Epoch [11/30], Step [600/600], Loss: 0.2836\n",
      "Epoch [11/30], Step [700/600], Loss: 0.5124\n",
      "Epoch [11/30], Step [800/600], Loss: 1.1645\n",
      "Epoch [11/30], Step [900/600], Loss: 0.2888\n",
      "Epoch [12/30], Step [100/600], Loss: 0.9345\n",
      "Epoch [12/30], Step [200/600], Loss: 1.0745\n",
      "Epoch [12/30], Step [300/600], Loss: 0.3739\n",
      "Epoch [12/30], Step [400/600], Loss: 0.4413\n",
      "Epoch [12/30], Step [500/600], Loss: 0.1341\n",
      "Epoch [12/30], Step [600/600], Loss: 0.2867\n",
      "Epoch [12/30], Step [700/600], Loss: 0.4994\n",
      "Epoch [12/30], Step [800/600], Loss: 1.1370\n",
      "Epoch [12/30], Step [900/600], Loss: 0.2811\n",
      "Epoch [13/30], Step [100/600], Loss: 0.9276\n",
      "Epoch [13/30], Step [200/600], Loss: 1.0853\n",
      "Epoch [13/30], Step [300/600], Loss: 0.3892\n",
      "Epoch [13/30], Step [400/600], Loss: 0.4287\n",
      "Epoch [13/30], Step [500/600], Loss: 0.1272\n",
      "Epoch [13/30], Step [600/600], Loss: 0.2815\n",
      "Epoch [13/30], Step [700/600], Loss: 0.4866\n",
      "Epoch [13/30], Step [800/600], Loss: 1.1294\n",
      "Epoch [13/30], Step [900/600], Loss: 0.2527\n",
      "Epoch [14/30], Step [100/600], Loss: 0.8543\n",
      "Epoch [14/30], Step [200/600], Loss: 1.0636\n",
      "Epoch [14/30], Step [300/600], Loss: 0.3875\n",
      "Epoch [14/30], Step [400/600], Loss: 0.4265\n",
      "Epoch [14/30], Step [500/600], Loss: 0.1240\n",
      "Epoch [14/30], Step [600/600], Loss: 0.2866\n",
      "Epoch [14/30], Step [700/600], Loss: 0.4628\n",
      "Epoch [14/30], Step [800/600], Loss: 1.1199\n",
      "Epoch [14/30], Step [900/600], Loss: 0.2520\n",
      "Epoch [15/30], Step [100/600], Loss: 0.8329\n",
      "Epoch [15/30], Step [200/600], Loss: 1.0592\n",
      "Epoch [15/30], Step [300/600], Loss: 0.3928\n",
      "Epoch [15/30], Step [400/600], Loss: 0.4112\n",
      "Epoch [15/30], Step [500/600], Loss: 0.1291\n",
      "Epoch [15/30], Step [600/600], Loss: 0.2543\n",
      "Epoch [15/30], Step [700/600], Loss: 0.4255\n",
      "Epoch [15/30], Step [800/600], Loss: 1.1296\n",
      "Epoch [15/30], Step [900/600], Loss: 0.2382\n",
      "Epoch [16/30], Step [100/600], Loss: 0.8057\n",
      "Epoch [16/30], Step [200/600], Loss: 1.0594\n",
      "Epoch [16/30], Step [300/600], Loss: 0.4026\n",
      "Epoch [16/30], Step [400/600], Loss: 0.4111\n",
      "Epoch [16/30], Step [500/600], Loss: 0.1192\n",
      "Epoch [16/30], Step [600/600], Loss: 0.2258\n",
      "Epoch [16/30], Step [700/600], Loss: 0.4151\n",
      "Epoch [16/30], Step [800/600], Loss: 1.1130\n",
      "Epoch [16/30], Step [900/600], Loss: 0.2302\n",
      "Epoch [17/30], Step [100/600], Loss: 0.7850\n",
      "Epoch [17/30], Step [200/600], Loss: 1.0482\n",
      "Epoch [17/30], Step [300/600], Loss: 0.3932\n",
      "Epoch [17/30], Step [400/600], Loss: 0.4133\n",
      "Epoch [17/30], Step [500/600], Loss: 0.1069\n",
      "Epoch [17/30], Step [600/600], Loss: 0.2198\n",
      "Epoch [17/30], Step [700/600], Loss: 0.4032\n",
      "Epoch [17/30], Step [800/600], Loss: 1.1299\n",
      "Epoch [17/30], Step [900/600], Loss: 0.2187\n",
      "Epoch [18/30], Step [100/600], Loss: 0.7217\n",
      "Epoch [18/30], Step [200/600], Loss: 1.0548\n",
      "Epoch [18/30], Step [300/600], Loss: 0.3988\n",
      "Epoch [18/30], Step [400/600], Loss: 0.4131\n",
      "Epoch [18/30], Step [500/600], Loss: 0.1039\n",
      "Epoch [18/30], Step [600/600], Loss: 0.1893\n",
      "Epoch [18/30], Step [700/600], Loss: 0.4124\n",
      "Epoch [18/30], Step [800/600], Loss: 1.0870\n",
      "Epoch [18/30], Step [900/600], Loss: 0.2199\n",
      "Epoch [19/30], Step [100/600], Loss: 0.7004\n",
      "Epoch [19/30], Step [200/600], Loss: 1.0269\n",
      "Epoch [19/30], Step [300/600], Loss: 0.3969\n",
      "Epoch [19/30], Step [400/600], Loss: 0.3920\n",
      "Epoch [19/30], Step [500/600], Loss: 0.1136\n",
      "Epoch [19/30], Step [600/600], Loss: 0.1716\n",
      "Epoch [19/30], Step [700/600], Loss: 0.3880\n",
      "Epoch [19/30], Step [800/600], Loss: 1.1096\n",
      "Epoch [19/30], Step [900/600], Loss: 0.2124\n",
      "Epoch [20/30], Step [100/600], Loss: 0.6620\n",
      "Epoch [20/30], Step [200/600], Loss: 1.0279\n",
      "Epoch [20/30], Step [300/600], Loss: 0.3938\n",
      "Epoch [20/30], Step [400/600], Loss: 0.3854\n",
      "Epoch [20/30], Step [500/600], Loss: 0.1041\n",
      "Epoch [20/30], Step [600/600], Loss: 0.1485\n",
      "Epoch [20/30], Step [700/600], Loss: 0.4004\n",
      "Epoch [20/30], Step [800/600], Loss: 1.1189\n",
      "Epoch [20/30], Step [900/600], Loss: 0.2042\n",
      "Epoch [21/30], Step [100/600], Loss: 0.6380\n",
      "Epoch [21/30], Step [200/600], Loss: 0.9917\n",
      "Epoch [21/30], Step [300/600], Loss: 0.3830\n",
      "Epoch [21/30], Step [400/600], Loss: 0.4143\n",
      "Epoch [21/30], Step [500/600], Loss: 0.0960\n",
      "Epoch [21/30], Step [600/600], Loss: 0.1419\n",
      "Epoch [21/30], Step [700/600], Loss: 0.3785\n",
      "Epoch [21/30], Step [800/600], Loss: 1.1134\n",
      "Epoch [21/30], Step [900/600], Loss: 0.2104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Step [100/600], Loss: 0.6330\n",
      "Epoch [22/30], Step [200/600], Loss: 0.9866\n",
      "Epoch [22/30], Step [300/600], Loss: 0.3630\n",
      "Epoch [22/30], Step [400/600], Loss: 0.3882\n",
      "Epoch [22/30], Step [500/600], Loss: 0.0907\n",
      "Epoch [22/30], Step [600/600], Loss: 0.1197\n",
      "Epoch [22/30], Step [700/600], Loss: 0.3689\n",
      "Epoch [22/30], Step [800/600], Loss: 1.0984\n",
      "Epoch [22/30], Step [900/600], Loss: 0.2076\n",
      "Epoch [23/30], Step [100/600], Loss: 0.5880\n",
      "Epoch [23/30], Step [200/600], Loss: 0.9851\n",
      "Epoch [23/30], Step [300/600], Loss: 0.3616\n",
      "Epoch [23/30], Step [400/600], Loss: 0.3897\n",
      "Epoch [23/30], Step [500/600], Loss: 0.0920\n",
      "Epoch [23/30], Step [600/600], Loss: 0.1245\n",
      "Epoch [23/30], Step [700/600], Loss: 0.3639\n",
      "Epoch [23/30], Step [800/600], Loss: 1.1154\n",
      "Epoch [23/30], Step [900/600], Loss: 0.2020\n",
      "Epoch [24/30], Step [100/600], Loss: 0.5418\n",
      "Epoch [24/30], Step [200/600], Loss: 0.9741\n",
      "Epoch [24/30], Step [300/600], Loss: 0.3223\n",
      "Epoch [24/30], Step [400/600], Loss: 0.3609\n",
      "Epoch [24/30], Step [500/600], Loss: 0.1006\n",
      "Epoch [24/30], Step [600/600], Loss: 0.1159\n",
      "Epoch [24/30], Step [700/600], Loss: 0.3453\n",
      "Epoch [24/30], Step [800/600], Loss: 1.1077\n",
      "Epoch [24/30], Step [900/600], Loss: 0.1906\n",
      "Epoch [25/30], Step [100/600], Loss: 0.5372\n",
      "Epoch [25/30], Step [200/600], Loss: 0.9756\n",
      "Epoch [25/30], Step [300/600], Loss: 0.3196\n",
      "Epoch [25/30], Step [400/600], Loss: 0.3640\n",
      "Epoch [25/30], Step [500/600], Loss: 0.1094\n",
      "Epoch [25/30], Step [600/600], Loss: 0.1097\n",
      "Epoch [25/30], Step [700/600], Loss: 0.3597\n",
      "Epoch [25/30], Step [800/600], Loss: 1.0894\n",
      "Epoch [25/30], Step [900/600], Loss: 0.1931\n",
      "Epoch [26/30], Step [100/600], Loss: 0.5199\n",
      "Epoch [26/30], Step [200/600], Loss: 0.9634\n",
      "Epoch [26/30], Step [300/600], Loss: 0.3132\n",
      "Epoch [26/30], Step [400/600], Loss: 0.3754\n",
      "Epoch [26/30], Step [500/600], Loss: 0.1060\n",
      "Epoch [26/30], Step [600/600], Loss: 0.1082\n",
      "Epoch [26/30], Step [700/600], Loss: 0.3522\n",
      "Epoch [26/30], Step [800/600], Loss: 1.1031\n",
      "Epoch [26/30], Step [900/600], Loss: 0.1978\n",
      "Epoch [27/30], Step [100/600], Loss: 0.5050\n",
      "Epoch [27/30], Step [200/600], Loss: 0.9526\n",
      "Epoch [27/30], Step [300/600], Loss: 0.2909\n",
      "Epoch [27/30], Step [400/600], Loss: 0.3687\n",
      "Epoch [27/30], Step [500/600], Loss: 0.1265\n",
      "Epoch [27/30], Step [600/600], Loss: 0.1041\n",
      "Epoch [27/30], Step [700/600], Loss: 0.3370\n",
      "Epoch [27/30], Step [800/600], Loss: 1.1229\n",
      "Epoch [27/30], Step [900/600], Loss: 0.1854\n",
      "Epoch [28/30], Step [100/600], Loss: 0.4939\n",
      "Epoch [28/30], Step [200/600], Loss: 0.9505\n",
      "Epoch [28/30], Step [300/600], Loss: 0.2818\n",
      "Epoch [28/30], Step [400/600], Loss: 0.3579\n",
      "Epoch [28/30], Step [500/600], Loss: 0.1283\n",
      "Epoch [28/30], Step [600/600], Loss: 0.1045\n",
      "Epoch [28/30], Step [700/600], Loss: 0.3291\n",
      "Epoch [28/30], Step [800/600], Loss: 1.1223\n",
      "Epoch [28/30], Step [900/600], Loss: 0.1849\n",
      "Epoch [29/30], Step [100/600], Loss: 0.4630\n",
      "Epoch [29/30], Step [200/600], Loss: 0.9533\n",
      "Epoch [29/30], Step [300/600], Loss: 0.2477\n",
      "Epoch [29/30], Step [400/600], Loss: 0.3471\n",
      "Epoch [29/30], Step [500/600], Loss: 0.1310\n",
      "Epoch [29/30], Step [600/600], Loss: 0.1001\n",
      "Epoch [29/30], Step [700/600], Loss: 0.3004\n",
      "Epoch [29/30], Step [800/600], Loss: 1.1322\n",
      "Epoch [29/30], Step [900/600], Loss: 0.1905\n",
      "Epoch [30/30], Step [100/600], Loss: 0.4336\n",
      "Epoch [30/30], Step [200/600], Loss: 0.9740\n",
      "Epoch [30/30], Step [300/600], Loss: 0.2092\n",
      "Epoch [30/30], Step [400/600], Loss: 0.3505\n",
      "Epoch [30/30], Step [500/600], Loss: 0.1499\n",
      "Epoch [30/30], Step [600/600], Loss: 0.0974\n",
      "Epoch [30/30], Step [700/600], Loss: 0.2932\n",
      "Epoch [30/30], Step [800/600], Loss: 1.1549\n",
      "Epoch [30/30], Step [900/600], Loss: 0.1849\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(custom_train_loader):  \n",
    "        images = Variable(images)         \n",
    "        labels = Variable(labels.long())\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(images)                           # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        if (i+1) % 100 == 0:                              # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, 6000//batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training the nerual network, we also need to load batches of test images and collect the outputs. The differences are that:\n",
    "(1) No loss & weights calculation\n",
    "(2) No wights update\n",
    "(3) Has correct prediction calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 8, 8, 0, 8, 5, 0, 7, 8])\n",
      "Accuracy of the network on the 10K test images: 86 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in custom_test_loader:\n",
    "    labels = labels.long()\n",
    "    images = Variable(images)\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels).sum()     # Increment the correct count\n",
    "\n",
    "print(labels)\n",
    "print('Accuracy of the network on the 10K test images: %d %%' % (100 * float(correct) / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(868)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained Model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(net.state_dict(), 'fnn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter('runs/mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = None\n",
    "for images, labels in test_loader:\n",
    "    imgs = images\n",
    "    break\n",
    "writer.add_graph(net,tensor_low_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net,tensor_low_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f6d4868f1d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'writer' is not defined"
     ]
    }
   ],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = scio.loadmat('low_dim_orig_test.mat')\n",
    "test_dat = test_data['low_dim'].reshape(10000)\n",
    "test_dat = np.array([i.reshape(18) for i in test_dat])\n",
    "test_dat = np.array([i[0].reshape(11) for i in test_dat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-55defa0c2fa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_dat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch2/hle/py3_env/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     42\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "test_dat.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.load('test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4279091 , 0.51963484, 0.52086675, 0.47780225, 0.5300925 ,\n",
       "       0.50020856, 0.30898094, 0.41586205, 0.2276364 , 0.5086932 ,\n",
       "       0.44525117], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_dim[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4279091 , 0.51963484, 0.52086675, 0.47780225, 0.5300925 ,\n",
       "       0.50020856, 0.30898094, 0.41586205, 0.2276364 , 0.5086932 ,\n",
       "       0.44525117], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_dim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.1296997e+00,  6.9629767e+02,  8.3496094e-02,  1.0404474e+02,\n",
       "       -3.2623260e+02, -2.8722021e+02,  8.4440002e+00, -1.9635437e+02,\n",
       "        5.7214197e+02,  1.8434105e+01,  1.3124892e+02], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dat[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
